#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äº§å“ç»ç†æ¨¡å—
åŠŸèƒ½ï¼šæ‹…ä»»äº§å“ç»ç†è§’è‰²ï¼Œè‡ªåŠ¨æµ‹è¯•ç¨‹åºã€æ£€æµ‹é—®é¢˜ã€æä¾›åé¦ˆ
"""

import asyncio
import time
import logging
import json
import subprocess
import sys
import os
import traceback
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from PIL import Image

logger = logging.getLogger(__name__)

class ProductManager:
    """äº§å“ç»ç†ç±» - è´Ÿè´£è´¨é‡ä¿è¯å’Œæµ‹è¯•"""
    
    def __init__(self, gpt_controller=None):
        self.gpt_controller = gpt_controller
        self.test_history = []
        self.issue_tracker = []
        self.quality_metrics = {
            "test_runs": 0,
            "issues_found": 0,
            "issues_resolved": 0,
            "success_rate": 0.0
        }
        
        # æµ‹è¯•ç­–ç•¥é…ç½®
        self.test_strategies = {
            "syntax_check": True,
            "import_check": True,
            "basic_run_check": True,
            "dependency_check": True,
            "error_simulation": True,
            "edge_case_testing": False  # é»˜è®¤å…³é—­æ·±åº¦æµ‹è¯•
        }
        
        # é—®é¢˜ä¸¥é‡çº§åˆ«
        self.severity_levels = {
            "critical": ["syntax error", "import error", "crash", "exception"],
            "high": ["performance", "memory", "timeout"],
            "medium": ["warning", "deprecation", "style"],
            "low": ["minor", "cosmetic", "suggestion"]
        }
    
    async def analyze_development_completion(self, screenshot: Image.Image, 
                                           completed_text: str, 
                                           project_path: str = ".") -> Dict[str, Any]:
        """åˆ†æå¼€å‘å®Œæˆæƒ…å†µå¹¶è¿›è¡Œè´¨é‡æ£€æŸ¥"""
        logger.info("ğŸ” äº§å“ç»ç†å¼€å§‹è´¨é‡æ£€æŸ¥...")
        
        analysis_result = {
            "timestamp": time.time(),
            "quality_score": 0.0,
            "issues": [],
            "recommendations": [],
            "test_results": {},
            "pm_feedback": "",
            "action_required": False
        }
        
        try:
            # 1. ä»£ç è´¨é‡æ£€æŸ¥
            code_quality = await self.check_code_quality(project_path)
            analysis_result["test_results"]["code_quality"] = code_quality
            
            # 2. è¿è¡Œæ—¶æµ‹è¯•
            runtime_test = await self.run_program_tests(project_path)
            analysis_result["test_results"]["runtime"] = runtime_test
            
            # 3. ä¾èµ–æ£€æŸ¥
            dependency_check = await self.check_dependencies(project_path)
            analysis_result["test_results"]["dependencies"] = dependency_check
            
            # 4. è®¡ç®—è´¨é‡åˆ†æ•°
            quality_score = self.calculate_quality_score(analysis_result["test_results"])
            analysis_result["quality_score"] = quality_score
            
            # 5. ç”Ÿæˆé—®é¢˜åˆ—è¡¨
            issues = self.extract_issues(analysis_result["test_results"])
            analysis_result["issues"] = issues
            
            # 6. ç”Ÿæˆæ”¹è¿›å»ºè®®
            recommendations = self.generate_recommendations(quality_score, issues)
            analysis_result["recommendations"] = recommendations
            
            # 7. äº§å“ç»ç†åé¦ˆ
            pm_feedback = self.generate_pm_feedback(quality_score, issues, completed_text)
            analysis_result["pm_feedback"] = pm_feedback
            
            # 8. åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥è¡ŒåŠ¨
            analysis_result["action_required"] = quality_score < 0.8 or len(issues) > 0
            
            # æ›´æ–°è´¨é‡æŒ‡æ ‡
            self.update_quality_metrics(analysis_result)
            
            logger.info(f"è´¨é‡æ£€æŸ¥å®Œæˆ - åˆ†æ•°: {quality_score:.2f}, é—®é¢˜: {len(issues)}ä¸ª")
            
        except Exception as e:
            logger.error(f"äº§å“ç»ç†åˆ†ææ—¶å‡ºé”™: {e}")
            analysis_result["pm_feedback"] = f"è´¨é‡æ£€æŸ¥é‡åˆ°æŠ€æœ¯é—®é¢˜: {str(e)}"
            analysis_result["action_required"] = True
        
        return analysis_result
    
    async def check_code_quality(self, project_path: str) -> Dict[str, Any]:
        """æ£€æŸ¥ä»£ç è´¨é‡"""
        logger.info("æ£€æŸ¥ä»£ç è´¨é‡...")
        
        quality_result = {
            "syntax_valid": True,
            "import_errors": [],
            "style_issues": [],
            "security_issues": [],
            "performance_issues": [],
            "overall_score": 1.0
        }
        
        try:
            # æŸ¥æ‰¾Pythonæ–‡ä»¶
            python_files = list(Path(project_path).rglob("*.py"))
            
            for py_file in python_files[:10]:  # é™åˆ¶æ£€æŸ¥æ–‡ä»¶æ•°é‡
                try:
                    # è¯­æ³•æ£€æŸ¥
                    with open(py_file, 'r', encoding='utf-8') as f:
                        code = f.read()
                    
                    # ä½¿ç”¨compileæ£€æŸ¥è¯­æ³•
                    try:
                        compile(code, str(py_file), 'exec')
                    except SyntaxError as e:
                        quality_result["syntax_valid"] = False
                        quality_result["import_errors"].append({
                            "file": str(py_file),
                            "error": str(e),
                            "line": e.lineno
                        })
                    
                    # æ£€æŸ¥å¸¸è§é—®é¢˜
                    await self.check_code_patterns(code, str(py_file), quality_result)
                    
                except Exception as e:
                    logger.debug(f"æ£€æŸ¥æ–‡ä»¶ {py_file} æ—¶å‡ºé”™: {e}")
                    continue
            
            # è®¡ç®—æ•´ä½“åˆ†æ•°
            quality_result["overall_score"] = self.calculate_code_quality_score(quality_result)
            
        except Exception as e:
            logger.error(f"ä»£ç è´¨é‡æ£€æŸ¥å‡ºé”™: {e}")
            quality_result["overall_score"] = 0.5
        
        return quality_result
    
    async def check_code_patterns(self, code: str, file_path: str, quality_result: Dict):
        """æ£€æŸ¥ä»£ç æ¨¡å¼å’Œå¸¸è§é—®é¢˜"""
        try:
            lines = code.split('\n')
            
            for i, line in enumerate(lines, 1):
                line_stripped = line.strip()
                
                # æ£€æŸ¥å®‰å…¨é—®é¢˜
                if any(pattern in line_stripped for pattern in ['eval(', 'exec(', 'os.system(']):
                    quality_result["security_issues"].append({
                        "file": file_path,
                        "line": i,
                        "issue": f"æ½œåœ¨å®‰å…¨é£é™©: {line_stripped[:50]}...",
                        "severity": "high"
                    })
                
                # æ£€æŸ¥æ€§èƒ½é—®é¢˜
                if 'import *' in line_stripped:
                    quality_result["performance_issues"].append({
                        "file": file_path,
                        "line": i,
                        "issue": "ä½¿ç”¨äº† import *ï¼Œå¯èƒ½å½±å“æ€§èƒ½",
                        "severity": "medium"
                    })
                
                # æ£€æŸ¥ä»£ç é£æ ¼
                if len(line) > 120:
                    quality_result["style_issues"].append({
                        "file": file_path,
                        "line": i,
                        "issue": f"è¡Œé•¿åº¦è¶…è¿‡120å­—ç¬¦ ({len(line)})",
                        "severity": "low"
                    })
        
        except Exception as e:
            logger.debug(f"æ£€æŸ¥ä»£ç æ¨¡å¼æ—¶å‡ºé”™: {e}")
    
    def calculate_code_quality_score(self, quality_result: Dict) -> float:
        """è®¡ç®—ä»£ç è´¨é‡åˆ†æ•°"""
        score = 1.0
        
        # è¯­æ³•é”™è¯¯ä¸¥é‡æ‰£åˆ†
        if not quality_result["syntax_valid"]:
            score -= 0.5
        
        # æ ¹æ®é—®é¢˜æ•°é‡æ‰£åˆ†
        score -= len(quality_result["security_issues"]) * 0.1
        score -= len(quality_result["performance_issues"]) * 0.05
        score -= len(quality_result["style_issues"]) * 0.01
        
        return max(0.0, score)
    
    async def run_program_tests(self, project_path: str) -> Dict[str, Any]:
        """è¿è¡Œç¨‹åºæµ‹è¯•"""
        logger.info("è¿è¡Œç¨‹åºæµ‹è¯•...")
        
        test_result = {
            "can_import": True,
            "can_run": True,
            "runtime_errors": [],
            "test_outputs": [],
            "execution_time": 0.0,
            "memory_usage": 0.0,
            "overall_score": 1.0
        }
        
        try:
            # æŸ¥æ‰¾ä¸»è¦çš„Pythonæ–‡ä»¶
            main_files = [
                "main.py", "app.py", "run.py", "start.py", 
                "__main__.py", "server.py"
            ]
            
            target_file = None
            for main_file in main_files:
                full_path = Path(project_path) / main_file
                if full_path.exists():
                    target_file = full_path
                    break
            
            if target_file:
                # æµ‹è¯•å¯¼å…¥
                import_test = await self.test_import(target_file)
                test_result.update(import_test)
                
                # æµ‹è¯•è¿è¡Œï¼ˆå¦‚æœå¯¼å…¥æˆåŠŸï¼‰
                if test_result["can_import"]:
                    run_test = await self.test_execution(target_file)
                    test_result.update(run_test)
            else:
                test_result["runtime_errors"].append("æœªæ‰¾åˆ°ä¸»è¦çš„Pythonå…¥å£æ–‡ä»¶")
                test_result["overall_score"] = 0.7
            
        except Exception as e:
            logger.error(f"ç¨‹åºæµ‹è¯•å‡ºé”™: {e}")
            test_result["runtime_errors"].append(f"æµ‹è¯•å¼‚å¸¸: {str(e)}")
            test_result["overall_score"] = 0.3
        
        return test_result
    
    async def test_import(self, file_path: Path) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
        result = {"can_import": True, "import_errors": []}
        
        try:
            # ä½¿ç”¨subprocessæµ‹è¯•å¯¼å…¥ï¼Œé¿å…å½±å“å½“å‰è¿›ç¨‹
            cmd = [sys.executable, "-c", f"import sys; sys.path.insert(0, '{file_path.parent}'); import {file_path.stem}"]
            
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                timeout=10
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode != 0:
                result["can_import"] = False
                result["import_errors"].append(stderr.decode('utf-8', errors='ignore'))
        
        except asyncio.TimeoutError:
            result["can_import"] = False
            result["import_errors"].append("å¯¼å…¥æµ‹è¯•è¶…æ—¶")
        except Exception as e:
            result["can_import"] = False
            result["import_errors"].append(f"å¯¼å…¥æµ‹è¯•å¼‚å¸¸: {str(e)}")
        
        return result
    
    async def test_execution(self, file_path: Path) -> Dict[str, Any]:
        """æµ‹è¯•ç¨‹åºæ‰§è¡Œ"""
        result = {
            "can_run": True,
            "execution_errors": [],
            "execution_time": 0.0,
            "test_outputs": []
        }
        
        try:
            start_time = time.time()
            
            # çŸ­æ—¶é—´è¿è¡Œæµ‹è¯•
            cmd = [sys.executable, str(file_path)]
            
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                timeout=5  # 5ç§’è¶…æ—¶
            )
            
            try:
                stdout, stderr = await process.communicate()
                execution_time = time.time() - start_time
                
                result["execution_time"] = execution_time
                result["test_outputs"].append(stdout.decode('utf-8', errors='ignore')[:500])
                
                if process.returncode != 0:
                    result["can_run"] = False
                    result["execution_errors"].append(stderr.decode('utf-8', errors='ignore'))
                
            except asyncio.TimeoutError:
                # è¶…æ—¶å¯èƒ½æ„å‘³ç€ç¨‹åºæ˜¯é•¿æœŸè¿è¡Œçš„æœåŠ¡ï¼Œè¿™æ˜¯æ­£å¸¸çš„
                process.terminate()
                result["test_outputs"].append("ç¨‹åºå¯åŠ¨æ­£å¸¸ï¼ˆé•¿æœŸè¿è¡ŒæœåŠ¡ï¼‰")
                result["execution_time"] = 5.0
        
        except Exception as e:
            result["can_run"] = False
            result["execution_errors"].append(f"æ‰§è¡Œæµ‹è¯•å¼‚å¸¸: {str(e)}")
        
        return result
    
    async def check_dependencies(self, project_path: str) -> Dict[str, Any]:
        """æ£€æŸ¥é¡¹ç›®ä¾èµ–"""
        logger.info("æ£€æŸ¥é¡¹ç›®ä¾èµ–...")
        
        dep_result = {
            "requirements_exist": False,
            "missing_packages": [],
            "outdated_packages": [],
            "dependency_conflicts": [],
            "overall_score": 1.0
        }
        
        try:
            # æ£€æŸ¥requirements.txt
            req_file = Path(project_path) / "requirements.txt"
            if req_file.exists():
                dep_result["requirements_exist"] = True
                
                # è¯»å–requirements
                with open(req_file, 'r', encoding='utf-8') as f:
                    requirements = f.read().strip().split('\n')
                
                # æ£€æŸ¥æ¯ä¸ªä¾èµ–
                for req in requirements:
                    if req.strip() and not req.strip().startswith('#'):
                        missing = await self.check_package_availability(req.strip())
                        if missing:
                            dep_result["missing_packages"].append(req.strip())
            
            # è®¡ç®—ä¾èµ–åˆ†æ•°
            dep_result["overall_score"] = self.calculate_dependency_score(dep_result)
            
        except Exception as e:
            logger.error(f"ä¾èµ–æ£€æŸ¥å‡ºé”™: {e}")
            dep_result["overall_score"] = 0.8
        
        return dep_result
    
    async def check_package_availability(self, package: str) -> bool:
        """æ£€æŸ¥åŒ…æ˜¯å¦å¯ç”¨"""
        try:
            # ç®€åŒ–çš„åŒ…æ£€æŸ¥ï¼ˆå¯ä»¥æ‰©å±•ï¼‰
            package_name = package.split('==')[0].split('>=')[0].split('<=')[0]
            
            cmd = [sys.executable, "-c", f"import {package_name}"]
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.DEVNULL,
                stderr=asyncio.subprocess.DEVNULL,
                timeout=5
            )
            
            await process.communicate()
            return process.returncode != 0  # True if missing
            
        except Exception:
            return True  # å‡è®¾ç¼ºå°‘
    
    def calculate_dependency_score(self, dep_result: Dict) -> float:
        """è®¡ç®—ä¾èµ–åˆ†æ•°"""
        score = 1.0
        
        if not dep_result["requirements_exist"]:
            score -= 0.2
        
        score -= len(dep_result["missing_packages"]) * 0.15
        score -= len(dep_result["dependency_conflicts"]) * 0.1
        
        return max(0.0, score)
    
    def calculate_quality_score(self, test_results: Dict) -> float:
        """è®¡ç®—æ•´ä½“è´¨é‡åˆ†æ•°"""
        scores = []
        
        if "code_quality" in test_results:
            scores.append(test_results["code_quality"]["overall_score"])
        
        if "runtime" in test_results:
            scores.append(test_results["runtime"]["overall_score"])
        
        if "dependencies" in test_results:
            scores.append(test_results["dependencies"]["overall_score"])
        
        return sum(scores) / len(scores) if scores else 0.5
    
    def extract_issues(self, test_results: Dict) -> List[Dict[str, Any]]:
        """æå–æ‰€æœ‰é—®é¢˜"""
        issues = []
        
        # ä»£ç è´¨é‡é—®é¢˜
        if "code_quality" in test_results:
            cq = test_results["code_quality"]
            
            if not cq["syntax_valid"]:
                for error in cq["import_errors"]:
                    issues.append({
                        "type": "syntax_error",
                        "severity": "critical",
                        "description": f"è¯­æ³•é”™è¯¯: {error['error']}",
                        "file": error.get("file", ""),
                        "line": error.get("line", 0)
                    })
            
            for sec_issue in cq["security_issues"]:
                issues.append({
                    "type": "security",
                    "severity": sec_issue["severity"],
                    "description": sec_issue["issue"],
                    "file": sec_issue["file"],
                    "line": sec_issue["line"]
                })
        
        # è¿è¡Œæ—¶é—®é¢˜
        if "runtime" in test_results:
            rt = test_results["runtime"]
            
            if not rt["can_import"]:
                for error in rt["import_errors"]:
                    issues.append({
                        "type": "import_error",
                        "severity": "critical",
                        "description": f"å¯¼å…¥é”™è¯¯: {error}",
                        "file": "",
                        "line": 0
                    })
            
            if not rt["can_run"]:
                for error in rt["execution_errors"]:
                    issues.append({
                        "type": "runtime_error",
                        "severity": "high",
                        "description": f"è¿è¡Œé”™è¯¯: {error}",
                        "file": "",
                        "line": 0
                    })
        
        # ä¾èµ–é—®é¢˜
        if "dependencies" in test_results:
            dep = test_results["dependencies"]
            
            for missing in dep["missing_packages"]:
                issues.append({
                    "type": "dependency",
                    "severity": "high",
                    "description": f"ç¼ºå°‘ä¾èµ–: {missing}",
                    "file": "requirements.txt",
                    "line": 0
                })
        
        return issues
    
    def generate_recommendations(self, quality_score: float, issues: List[Dict]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if quality_score < 0.5:
            recommendations.append("ğŸš¨ ä»£ç è´¨é‡ä¸¥é‡ä¸è¾¾æ ‡ï¼Œéœ€è¦å…¨é¢é‡æ„")
        elif quality_score < 0.8:
            recommendations.append("âš ï¸ ä»£ç è´¨é‡éœ€è¦æ”¹è¿›ï¼Œå»ºè®®ä¼˜åŒ–æ ¸å¿ƒé—®é¢˜")
        
        # æŒ‰é—®é¢˜ç±»å‹åˆ†ç»„å»ºè®®
        error_types = {}
        for issue in issues:
            issue_type = issue["type"]
            if issue_type not in error_types:
                error_types[issue_type] = []
            error_types[issue_type].append(issue)
        
        for issue_type, type_issues in error_types.items():
            count = len(type_issues)
            if issue_type == "syntax_error":
                recommendations.append(f"ğŸ”´ ä¿®å¤ {count} ä¸ªè¯­æ³•é”™è¯¯ï¼ˆä¼˜å…ˆçº§ï¼šæœ€é«˜ï¼‰")
            elif issue_type == "import_error":
                recommendations.append(f"ğŸŸ  è§£å†³ {count} ä¸ªå¯¼å…¥é—®é¢˜ï¼ˆæ£€æŸ¥æ¨¡å—è·¯å¾„å’Œä¾èµ–ï¼‰")
            elif issue_type == "runtime_error":
                recommendations.append(f"ğŸŸ¡ ä¿®å¤ {count} ä¸ªè¿è¡Œæ—¶é”™è¯¯")
            elif issue_type == "dependency":
                recommendations.append(f"ğŸ“¦ å®‰è£… {count} ä¸ªç¼ºå¤±çš„ä¾èµ–åŒ…")
            elif issue_type == "security":
                recommendations.append(f"ğŸ”’ å¤„ç† {count} ä¸ªå®‰å…¨é£é™©")
        
        if not recommendations:
            recommendations.append("âœ… ä»£ç è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘æ€§èƒ½ä¼˜åŒ–å’ŒåŠŸèƒ½æ‰©å±•")
        
        return recommendations
    
    def generate_pm_feedback(self, quality_score: float, issues: List[Dict], completed_text: str) -> str:
        """ç”Ÿæˆäº§å“ç»ç†åé¦ˆ"""
        
        # ä¸»åŠ›æ“ç›˜æ‰‹è§’åº¦çš„çŠ€åˆ©åˆ†æ
        if quality_score >= 0.9:
            pm_feedback = f"""ğŸ¯ **äº§å“ç»ç†è´¨é‡è¯„ä¼°** (åˆ†æ•°: {quality_score:.1f}/1.0)

âœ… **è´¨é‡è¯„çº§: ä¼˜ç§€**
ä»ä¸»åŠ›æ“ç›˜æ‰‹è§’åº¦çœ‹ï¼Œè¿™æ¬¡å¼€å‘å±•ç°äº†çœŸæ­£çš„ä¸“ä¸šæ°´å‡†ï¼ä»£ç è´¨é‡è¾¾åˆ°ç”Ÿäº§çº§åˆ«ï¼Œå¯ä»¥æ”¾å¿ƒæŠ•å…¥å®æˆ˜ã€‚

ğŸ’¡ **æˆ˜ç•¥å»ºè®®**: å½“å‰ç‰ˆæœ¬å·²å…·å¤‡æ ¸å¿ƒç«äº‰åŠ›ï¼Œå»ºè®®ï¼š
1. ç«‹å³è¿›è¡Œå‹åŠ›æµ‹è¯•éªŒè¯ç¨³å®šæ€§  
2. éƒ¨ç½²åˆ°çœŸå®ç¯å¢ƒè·å–æ•°æ®åé¦ˆ
3. å‡†å¤‡ä¸‹ä¸€é˜¶æ®µçš„åŠŸèƒ½æ‰©å±•

è¿™ç§è´¨é‡æ°´å¹³åœ¨å¸‚åœºä¸Šèƒ½å¤Ÿå æ®ä¸»å¯¼åœ°ä½ï¼"""

        elif quality_score >= 0.7:
            critical_count = len([i for i in issues if i["severity"] == "critical"])
            pm_feedback = f"""âš ï¸ **äº§å“ç»ç†è´¨é‡è¯„ä¼°** (åˆ†æ•°: {quality_score:.1f}/1.0)

ğŸŸ¡ **è´¨é‡è¯„çº§: è‰¯å¥½ä½†éœ€ä¼˜åŒ–**
ä½œä¸ºç»éªŒä¸°å¯Œçš„æ“ç›˜æ‰‹ï¼Œæˆ‘çœ‹åˆ°äº†æ½œåŠ›ï¼Œä½†ä¹Ÿå‘ç°äº†é£é™©ç‚¹ã€‚å½“å‰ç‰ˆæœ¬å¯ä»¥å·¥ä½œï¼Œä½†è¿˜ä¸å¤Ÿç¨³å®šã€‚

ğŸ” **å‘ç°é—®é¢˜**: {len(issues)}ä¸ªé—®é¢˜ï¼Œå…¶ä¸­{critical_count}ä¸ªä¸¥é‡é—®é¢˜
ğŸ’ª **æ”¹è¿›é‡ç‚¹**: 
1. ä¼˜å…ˆè§£å†³æ‰€æœ‰ä¸¥é‡é—®é¢˜ï¼ˆé¿å…ç”Ÿäº§äº‹æ•…ï¼‰
2. å®Œå–„é”™è¯¯å¤„ç†æœºåˆ¶
3. å¢å¼ºä»£ç ç¨³å®šæ€§

ä¿®å¤è¿™äº›é—®é¢˜åï¼Œæˆ‘ä»¬å°±èƒ½ä»"èƒ½ç”¨"å‡çº§åˆ°"å¥½ç”¨"ï¼"""

        else:
            critical_count = len([i for i in issues if i["severity"] == "critical"])
            pm_feedback = f"""ğŸš¨ **äº§å“ç»ç†è´¨é‡è¯„ä¼°** (åˆ†æ•°: {quality_score:.1f}/1.0)

ğŸ”´ **è´¨é‡è¯„çº§: éœ€è¦é‡å¤§æ”¹è¿›**  
å¦ç‡åœ°è¯´ï¼Œå½“å‰ç‰ˆæœ¬è¿˜ä¸é€‚åˆæŠ•å…¥ä½¿ç”¨ã€‚ä½œä¸ºè´Ÿè´£ä»»çš„äº§å“ç»ç†ï¼Œæˆ‘å¿…é¡»é˜»æ­¢è¿™ç§è´¨é‡çš„ä»£ç è¿›å…¥ç”Ÿäº§ç¯å¢ƒã€‚

âš¡ **ç´§æ€¥é—®é¢˜**: {len(issues)}ä¸ªé—®é¢˜ï¼Œ{critical_count}ä¸ªè‡´å‘½é”™è¯¯
ğŸ¯ **ç«‹å³è¡ŒåŠ¨**: 
1. åœæ­¢æ–°åŠŸèƒ½å¼€å‘ï¼Œå…¨åŠ›ä¿®å¤åŸºç¡€é—®é¢˜
2. å»ºç«‹ä»£ç å®¡æŸ¥æµç¨‹
3. å¢åŠ å•å…ƒæµ‹è¯•è¦†ç›–

è®°ä½ï¼šåœ¨è‚¡å¸‚ä¸­ï¼Œè´¨é‡ä¸è¿‡å…³çš„äº§å“ä¼šè¢«å¸‚åœºæ— æƒ…æ·˜æ±°ï¼è®©æˆ‘ä»¬å…ˆæŠŠåŸºç¡€æ‰“ç‰¢ã€‚"""

        # æ·»åŠ å…·ä½“é—®é¢˜æ‘˜è¦
        if issues:
            pm_feedback += f"\n\nğŸ“‹ **é—®é¢˜æ¸…å•**:\n"
            for i, issue in enumerate(issues[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ªé—®é¢˜
                pm_feedback += f"{i}. [{issue['severity'].upper()}] {issue['description']}\n"
            
            if len(issues) > 5:
                pm_feedback += f"... è¿˜æœ‰ {len(issues) - 5} ä¸ªé—®é¢˜éœ€è¦è§£å†³\n"
        
        return pm_feedback
    
    def update_quality_metrics(self, analysis_result: Dict):
        """æ›´æ–°è´¨é‡æŒ‡æ ‡"""
        self.quality_metrics["test_runs"] += 1
        self.quality_metrics["issues_found"] += len(analysis_result["issues"])
        
        # è®¡ç®—æˆåŠŸç‡
        if analysis_result["quality_score"] >= 0.8:
            self.quality_metrics["issues_resolved"] += 1
        
        self.quality_metrics["success_rate"] = (
            self.quality_metrics["issues_resolved"] / self.quality_metrics["test_runs"]
            if self.quality_metrics["test_runs"] > 0 else 0.0
        )
    
    def get_quality_summary(self) -> Dict[str, Any]:
        """è·å–è´¨é‡æ‘˜è¦"""
        return {
            "metrics": self.quality_metrics,
            "recent_issues": self.issue_tracker[-10:],
            "test_strategies": self.test_strategies
        }
    
    def generate_master_feedback(self, quality_report: Dict[str, Any], gpt_analysis: Optional[Dict] = None) -> str:
        """ç”Ÿæˆä¸»åé¦ˆ - å…¼å®¹æ–¹æ³•"""
        try:
            quality_score = quality_report.get("quality_score", 0.0)
            issues = quality_report.get("issues", [])
            completed_text = quality_report.get("completed_text", "")
            
            # ä½¿ç”¨ç°æœ‰çš„generate_pm_feedbackæ–¹æ³•ç”ŸæˆåŸºç¡€åé¦ˆ
            base_feedback = self.generate_pm_feedback(quality_score, issues, completed_text)
            
            # å¦‚æœæœ‰GPTåˆ†æç»“æœï¼Œåˆå¹¶åé¦ˆ
            if gpt_analysis:
                gpt_reasoning = gpt_analysis.get("reasoning", "")
                gpt_recommendations = gpt_analysis.get("recommendations", [])
                
                combined_feedback = f"""{base_feedback}

---

ğŸ¤– **AIæ·±åº¦åˆ†æè¡¥å……**:
{gpt_reasoning}

ğŸ’¡ **AIå»ºè®®**:
{chr(10).join([f"â€¢ {rec}" for rec in gpt_recommendations[:3]])}"""
                
                return combined_feedback
            else:
                return base_feedback
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆä¸»åé¦ˆæ—¶å‡ºé”™: {e}")
            return f"âš ï¸ äº§å“ç»ç†åé¦ˆç”Ÿæˆé‡åˆ°é—®é¢˜: {str(e)}" 